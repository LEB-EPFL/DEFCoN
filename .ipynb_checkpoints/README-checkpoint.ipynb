{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFCoN package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick workflow guide to DEFCoN (Density Estimation Fully Convolutional Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection\n",
    "### TrainingSet\n",
    "A training set for DEFCoN is made of input SMLM images, with their segmentation masks and density maps built from the ground truth of the fluorophores. The training images are built using SASS. The simulation must output the generated tiff stack as well as a *frame logger* csv file (FrameLogger class in SASS). The TrainingSet class is used to store the data. It is inherited from an h5py File."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following creates a TrainingSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from DEFCoN.datasets import TrainingSet\n",
    "\n",
    "h5_path = 'example/training/my_training_set.h5'\n",
    "# Create h5 file, replace if exists\n",
    "my_training_set = TrainingSet(h5_path, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the TrainingSet is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example/training/my_training_set.h5\n",
      "/\n",
      "    input/\n",
      "    target/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_training_set.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods add_dataset is used to add the input images to the TrainingSet, and build their density maps and segmentation masks from the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "dataset_name = 'first_dataset'\n",
    "my_training_set.add_dataset(name=dataset_name,\n",
    "                            input_file='example/data_1/generated_stack.tif',\n",
    "                            frame_logger='example/data_1/frame_logger.csv')\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TrainingSet now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example/training/my_training_set.h5\n",
      "/\n",
      "    input/\n",
      "        first_dataset: shape = (1500, 64, 64, 1), dtype = float64\n",
      "    seg_map/\n",
      "        first_dataset: shape = (1500, 64, 64, 1), dtype = int32\n",
      "    target/\n",
      "        first_dataset: shape = (1500, 64, 64, 1), dtype = float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_training_set.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a second dataset, the TrainingSet becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example/training/my_training_set.h5\n",
      "/\n",
      "    input/\n",
      "        first_dataset: shape = (1500, 64, 64, 1), dtype = float64\n",
      "        second_dataset: shape = (1500, 64, 64, 1), dtype = float64\n",
      "    seg_map/\n",
      "        first_dataset: shape = (1500, 64, 64, 1), dtype = int32\n",
      "        second_dataset: shape = (1500, 64, 64, 1), dtype = int32\n",
      "    target/\n",
      "        first_dataset: shape = (1500, 64, 64, 1), dtype = float64\n",
      "        second_dataset: shape = (1500, 64, 64, 1), dtype = float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'second_dataset'\n",
    "my_training_set.add_dataset(name=dataset_name,\n",
    "                            input_file='example/data_2/generated_stack.tif',\n",
    "                            frame_logger='example/data_2/frame_logger.csv')\n",
    "\n",
    "clear_output()\n",
    "my_training_set.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always close the TrainingSet when you're done using it. It's a limitation of the HDF5 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_training_set.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compact TrainingSet\n",
    "To train faster, a TrainingSet containing multiple datasets can be compacted in a single large dataset, provided that _all input images have the same dimensions_.The images can be augmented (by random brightness changes) and shuffled in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compacting input data...\n",
      "    first_dataset...\n",
      "    second_dataset...\n",
      "Augmenting data...\n",
      "Compacting seg_map data...\n",
      "    first_dataset...\n",
      "    second_dataset...\n",
      "Compacting target data...\n",
      "    first_dataset...\n",
      "    second_dataset...\n"
     ]
    }
   ],
   "source": [
    "from DEFCoN.datasets import compact_set\n",
    "\n",
    "# This time, we open my_training_set in read_only mode\n",
    "my_training_set = TrainingSet('example/training/my_training_set.h5', 'r')\n",
    "\n",
    "# Output a compact training set named 'my_compact_set.h5'\n",
    "compact_set(my_training_set,\n",
    "            output_file='example/training/my_compact_set.h5',\n",
    "            shuffle=True,\n",
    "            augment=True)\n",
    "\n",
    "# Close my_training_set\n",
    "my_training_set.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "example/training/my_compact_set.h5\n",
      "/\n",
      "    input/\n",
      "        input: shape = (3000, 64, 64, 1), dtype = float64\n",
      "    seg_map/\n",
      "        seg_map: shape = (3000, 64, 64, 1), dtype = int32\n",
      "    target/\n",
      "        target: shape = (3000, 64, 64, 1), dtype = float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_compact_set = TrainingSet('example/training/my_compact_set.h5')\n",
    "my_compact_set.summary()\n",
    "my_compact_set.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The best method for training DEFCoN is by traning first the segmentation network, then the density network. The current architecture can be found in DEFCoN.models. The package uses the FCN class to add methods to the Keras Model class. The most efficient method for training is to train on a compact set of augmented and shuffled images. It is implemented in the \"train\" method from the DEFCoN.training module. The parameters are input using a config ini file. Taking a look at our example config file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Configuration file for training DEFCoN\n",
    "[General]\n",
    "# Name of the keras model to save\n",
    "ModelName = my_trained_model.h5\n",
    "# Path to the TrainingSet h5 file\n",
    "TrainingSetPath = ./../my_compact_set.h5\n",
    "# Output directory\n",
    "OutputDir = trained_models/\n",
    "# Directory to store the intermediary weights\n",
    "WeightDir = weights/\n",
    "\n",
    "[SegNet]\n",
    "BatchSize = 64\n",
    "NumEpochs = 1\n",
    "ValidationSplit = 0.1\n",
    "AdamLR = 0.001\n",
    "\n",
    "[DensityNet]\n",
    "LambdaFactor = 0.01\n",
    "BatchSize = 64\n",
    "NumEpochs = 1\n",
    "ValidationSplit = 0.1\n",
    "AdamLR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first section just deals with the paths of the TrainingSet and the output model and weights. Note that the paths to the directories and training set file are **relative to the config file**. The other sections let the user tune the parameters of the training. To train a model, simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from DEFCoN import training\n",
    "\n",
    "training.train('example/training/config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script can also be run from the command line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python training.py path_to_config_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Predictions can be made on arrays (TrainingSet format) or on images. DEFCoN.networks.FCN is a class inheriting methods from the Keras Model class and with added methods for prediction and serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from DEFCoN.networks import FCN\n",
    "from DEFCoN.datasets import TrainingSet\n",
    "import numpy as np\n",
    "\n",
    "# Import the trained model from the saved h5 file\n",
    "model = FCN.from_file('example/training/trained_models/my_trained_model.h5')\n",
    "\n",
    "# Predict from array\n",
    "my_training_set = TrainingSet('example/training/my_training_set.h5', 'r')\n",
    "array = np.array(my_training_set['input/first_dataset'])\n",
    "my_training_set.close()\n",
    "\n",
    "y_pred = model.predict(array)\n",
    "\n",
    "# Predict from image stack\n",
    "y_pred = model.predict_tiff('example/data_1/generated_stack.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is a stack of density map, as a numpy array with 4 dimensions: (N_frames, x, y, 1). The last dimension is the number of channels (1 because it is grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 64, 64, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving for Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained DEFCoN model can be exported to be used in the Java implementation of DEFCoN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from DEFCoN.networks import FCN\n",
    "\n",
    "# Import the trained model from the saved h5 file\n",
    "model = FCN.from_file('example/training/trained_models/my_trained_model.h5')\n",
    "\n",
    "# Save it for serving\n",
    "model.save_tf_model('example/serving/tf_density_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting folder can be directly placed in the DEFCoN plugin directory in FIJI\n",
    "\n",
    "\n",
    "Fiji.app/plugins/DEFCoN/tf_density_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum local count\n",
    "Trained DEFCoN model can be converted to allow for maximum count prediction by adding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "hist_norm (Lambda)           (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv_seg_1 (Conv2D)          (None, None, None, 16)    160       \n",
      "_________________________________________________________________\n",
      "conv_stride_1 (Conv2D)       (None, None, None, 16)    2320      \n",
      "_________________________________________________________________\n",
      "conv_seg_2 (Conv2D)          (None, None, None, 32)    4640      \n",
      "_________________________________________________________________\n",
      "conv_stride_2 (Conv2D)       (None, None, None, 32)    9248      \n",
      "_________________________________________________________________\n",
      "conv_seg_3 (Conv2D)          (None, None, None, 64)    18496     \n",
      "_________________________________________________________________\n",
      "dropout_seg (Dropout)        (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "deconv_seg_1 (Conv2DTranspos (None, None, None, 8)     4616      \n",
      "_________________________________________________________________\n",
      "decon_seg_2 (Conv2DTranspose (None, None, None, 8)     584       \n",
      "_________________________________________________________________\n",
      "seg (Conv2D)                 (None, None, None, 1)     8         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, None, None, 16)    160       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, None, None, 16)    2320      \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, None, None, 32)    4640      \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, None, None, 32)    9248      \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, None, None, 64)    51264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "deconv_1 (Conv2DTranspose)   (None, None, None, 8)     4616      \n",
      "_________________________________________________________________\n",
      "deconv_2 (Conv2DTranspose)   (None, None, None, 8)     584       \n",
      "_________________________________________________________________\n",
      "density (Conv2D)             (None, None, None, 1)     8         \n",
      "=================================================================\n",
      "Total params: 112,912\n",
      "Trainable params: 72,840\n",
      "Non-trainable params: 40,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from DEFCoN.networks import FCN\n",
    "\n",
    "# Import the trained model from the saved h5 file\n",
    "model = FCN.from_file('example/training/trained_models/my_trained_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "hist_norm (Lambda)           (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv_seg_1 (Conv2D)          (None, None, None, 16)    160       \n",
      "_________________________________________________________________\n",
      "conv_stride_1 (Conv2D)       (None, None, None, 16)    2320      \n",
      "_________________________________________________________________\n",
      "conv_seg_2 (Conv2D)          (None, None, None, 32)    4640      \n",
      "_________________________________________________________________\n",
      "conv_stride_2 (Conv2D)       (None, None, None, 32)    9248      \n",
      "_________________________________________________________________\n",
      "conv_seg_3 (Conv2D)          (None, None, None, 64)    18496     \n",
      "_________________________________________________________________\n",
      "dropout_seg (Dropout)        (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "deconv_seg_1 (Conv2DTranspos (None, None, None, 8)     4616      \n",
      "_________________________________________________________________\n",
      "decon_seg_2 (Conv2DTranspose (None, None, None, 8)     584       \n",
      "_________________________________________________________________\n",
      "seg (Conv2D)                 (None, None, None, 1)     8         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, None, None, 16)    160       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, None, None, 16)    2320      \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, None, None, 32)    4640      \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv2D)              (None, None, None, 32)    9248      \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv2D)              (None, None, None, 64)    51264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "deconv_1 (Conv2DTranspose)   (None, None, None, 8)     4616      \n",
      "_________________________________________________________________\n",
      "deconv_2 (Conv2DTranspose)   (None, None, None, 8)     584       \n",
      "_________________________________________________________________\n",
      "density (Conv2D)             (None, None, None, 1)     8         \n",
      "_________________________________________________________________\n",
      "av_pool (AveragePooling2D)   (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "mult (Lambda)                (None, None, None, 1)     0         \n",
      "_________________________________________________________________\n",
      "max_count_output (GlobalMaxP (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 112,912\n",
      "Trainable params: 72,840\n",
      "Non-trainable params: 40,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Adding prediction layer for max local count with 7x7 windows\n",
    "model.density_to_max_count(pool=(7,7))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model can then be served for Java:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_tf_model('example/serving/tf_max_count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
